import geopandas as gpd
import pandas as pd
import matplotlib.pyplot as plt
from shapely.geometry import Point
# Load Earthquake Vulnerability Zones Shapefile ---
path = "../data/bgd_nhr_earthquake_barc"
gdf = gpd.read_file(path)

print("Earthquake Zones CRS:", gdf.crs)
print(gdf.head())

# Load Roads CSV and Convert to GeoDataFrame ---
roads_path = "../data/_roads3.csv"
df_roads = pd.read_csv(roads_path)

# Rename if needed for clarity
df_roads = df_roads.rename(columns={"lat": "latitude", "lon": "longitude"})

# Create geometry column from lat/lon
geometry = [Point(xy) for xy in zip(df_roads["longitude"], df_roads["latitude"])]
gdf_roads = gpd.GeoDataFrame(df_roads, geometry=geometry, crs="EPSG:4326")  # WGS 84

# Reproject Roads to Match Shapefile CRS ---
gdf_roads = gdf_roads.to_crs(gdf.crs)

# Perform Spatial Join ---
joined = gpd.sjoin(gdf_roads, gdf, how="left", predicate="within")

# Select Relevant Columns (Optional) ---
relevant_columns = df_roads.columns.tolist() + ['ZONE', 'CO_EFFIC', 'EARTHQUF_I']
result = joined[relevant_columns]

# Normalize the EARTHQUF_I score using min-max normalization
min_score = result['EARTHQUF_I'].min()
max_score = result['EARTHQUF_I'].max()

result['normalized_score'] = (result['EARTHQUF_I'] - min_score) / (max_score - min_score)

# Replace the original EARTHQUF_I scores with the normalized scores
result['EARTHQUF_I'] = result['normalized_score']

# 6. Export or Analyze ---
print(result.head())
result.to_csv("../data/processed_data/roads_with_earthquake_vulnerability.csv", index=False)

average_score_per_road = result.groupby("road")["EARTHQUF_I"].mean().reset_index()
average_score_per_road = average_score_per_road.rename(columns={"EARTHQUF_I": "avg_earthquake_score"})

# Display the result
print(average_score_per_road)
#average_score_per_road.value_counts()
average_score_per_road.to_csv("../data/processed_data/avg_earthquake_score.csv", index=False)



# Load bridges Excel
df_bridges = pd.read_excel("../data/BMMS_overview.xlsx", sheet_name="BMMS_overview")

# Create GeoDataFrame from lat/lon
df_bridges = df_bridges.rename(columns={"lat": "latitude", "lon": "longitude"})
geometry = [Point(xy) for xy in zip(df_bridges["longitude"], df_bridges["latitude"])]
gdf_bridges = gpd.GeoDataFrame(df_bridges, geometry=geometry, crs="EPSG:4326")

# Reproject to match the shapefile CRS
gdf_bridges = gdf_bridges.to_crs(gdf.crs)

# Spatial join to get earthquake zone attributes
joined_eq_bridges = gpd.sjoin(gdf_bridges, gdf, how="left", predicate="within")

# Calculate the average earthquake score per LRPName
average_eq_score_per_bridge = (
    joined_eq_bridges.groupby("LRPName")["EARTHQUF_I"]
    .mean()
    .reset_index()
    .rename(columns={"EARTHQUF_I": "avg_earthquake_score"})
)

# Normalize the average earthquake score using min-max normalization
min_score = average_eq_score_per_bridge["avg_earthquake_score"].min()
max_score = average_eq_score_per_bridge["avg_earthquake_score"].max()

average_eq_score_per_bridge["avg_earthquake_score"] = (
    (average_eq_score_per_bridge["avg_earthquake_score"] - min_score) / (max_score - min_score)
)

# Save the results to a CSV file
output_path = "../data/processed_data/avg_bridge_earthquake_score.csv"
average_eq_score_per_bridge.to_csv(output_path, index=False)

# Display the final table
print(average_eq_score_per_bridge)
print(f"The normalized average earthquake scores per LRPName have been saved to '{output_path}'.")
